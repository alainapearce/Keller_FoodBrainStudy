#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script was created to summarize motion info from censorysummary files generated by p2_create_censor_files. 

Written by Bari Fuchs in Summer 2023

Copyright (C) 2023 Bari Fuchs

     This program is free software: you can redistribute it and/or modify
     it under the terms of the GNU General Public License as published by
     the Free Software Foundation, either version 3 of the License, or
     (at your option) any later version.

     This program is distributed in the hope that it will be useful,
     but WITHOUT ANY WARRANTY; without even the implied warranty of
     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
     GNU General Public License for more details.

     You should have received a copy of the GNU General Public License
     along with this program.  If not, see <https://www.gnu.org/licenses/>.
     
This script is not guaranteed to work for new data or under new directory 
configurations, however, it should work if no changes are made to directories
or raw data configurations.

@author: baf44
"""

#set up packages    
import numpy as np
import pandas as pd
import os
from pathlib import Path
import re

##############################################################################
####                                                                      ####
####                             Core Script                              ####
####                                                                      ####
##############################################################################


def _get_summary_file(bids_fmriprep_path, censor_str, sub, overwrite):
    """Function to import or generate task-foodcue_censorsummary_* file. 
        Files will be imported if censorsummary file with specified CensorStr exists, otherwise they will be generated. 
        If subject is already in censorsummary database and Overwrite = False, exception will be raised. 
        If subject is already in censorsummary database and Overwrite != False, subject will be removed from summary database.

    Inputs:
        fMRIpreppath (list) - path to import file from
        censor_str (str) - string that defines TR censor criteria 
        sub (str) - subject ID
        overwrite (bool) - True or False for overwriting subject data in censor summary files
        
    Outputs:
        RegressPardat (pandas dataframe) - will contain 1 column per regressor variable and (number confound files * length of 1 confound file) rows
    """

    # Set path to summary file
    censor_summary_path = Path(bids_fmriprep_path).joinpath('task-foodcue_F31-censorsummary.tsv')

    ### Manage censor_summary_path ###
    if censor_summary_path.is_file(): # if database exists

        # import database --- converting 'sub' to string will maintain leading zeros
        CenSum_allPar = pd.read_csv(str(censor_summary_path), sep = '\t', converters={'sub': lambda x: str(x)})

        # check to see if subject already in database for given censor_str
        if CenSum_allPar[(CenSum_allPar['sub'] == sub) & (CenSum_allPar['censor_str'] == censor_str)].shape[0] > 0:
            if overwrite is False:
                print("sub_" + sub + " already in task-foodcue_F31-censorsummary.tsv for given censor_str. Use overwrite = True to rerun")
                raise Exception()
            else: #overwrite is true
                # remove row from censor_summary_path
                CenSum_allPar = CenSum_allPar.drop(CenSum_allPar[(CenSum_allPar['sub'] == sub) & (CenSum_allPar['censor_str'] == censor_str)].index)

    # if database does not exist
    else:
        # create new dataframe 
        CenSum_allPar = pd.DataFrame(np.zeros((0, 5)))
        CenSum_allPar.columns = ['sub','censor_str', 'n_total_trs', 'n_total_trs_uncensored', 'n_food_trs_uncensored']

    return(CenSum_allPar)

def _get_food_onsetTRs(eventsfiles):

    # set length of TR
    TR = 2
    
    # initialize onsets dictionary
    par_food_onset_TRs = {}

    for file in eventsfiles:

        #load data
        foodcue_RunDat = pd.read_csv(str(file), sep = '\t', encoding = 'utf-8-sig', engine='python')

        # select only variables interested in using for processed data
        foodcue_RunDat = foodcue_RunDat[['sub', 'ses', 'experiment_name' ,'block', 'trial', 'condition', 'stimslide_onsettime', 'stimslide_onsettoonsettime', 'onset', 'duration']]
    
        # rename columns (note: block becomes run, trial becomes block)
        foodcue_RunDat.columns = ['sub', 'ses', 'experiment_name', 'run', 'block', 'condition', 'stim_onset', 'stim_onset2onset', 'onset', 'duration']

        ## Get run number   
        run_num = foodcue_RunDat['run'].iloc[0]

        #get all non-duplicate blocks in run
        blocks = foodcue_RunDat['block'].unique()

        #loop through blocks
        for b in blocks:

            #subset block data from foodcue_data
            block_dat = foodcue_RunDat[foodcue_RunDat['block'] == b]

            # Add food block onset TR to dictionary -- TR will be onset time / TR 
            b_condition = block_dat['condition'].iloc[0]
            if "High" in b_condition or "Low" in b_condition:
                if run_num in par_food_onset_TRs:
                    par_food_onset_TRs[run_num].append(block_dat['onset'].iloc[0]/TR)
                else:
                    par_food_onset_TRs[run_num] = [block_dat['onset'].iloc[0]/TR]

    return(par_food_onset_TRs)

def _gen_food_TR_list(par_food_onset_TRs, confound_files):

    """Function to generate r_int_list and block_onsets_TR_dict based on original onset files
    Inputs:
        onsets_Pardat (dictionary): keys are run numbers, values are food block onsets
        confound_files (list) 
    Outputs:
        r_int_list (list) - a list of 1s and 0s equal to the length of a run -- 0 = TR is of non-interest, 1 = TR of interest
    """

    food_TR_list = []

    # sort confound_files
    confound_files.sort()
    
    # loop though confound_files
    for i in range(len(confound_files)):

        #load data
        confound_dat = pd.read_csv(str(confound_files[i]), sep = '\t', encoding = 'utf-8-sig', engine='python')

        # Make a list of 1s and 0s equal to the length of a run -- 0 = TR is of non-interest, 1 = TR of interest
        run_food_TR_list = [0] * len(confound_dat) # Make a list of 0s equal to the length of confound_dat

        # set run number
        run_num = i + 1

        # loop through onsets for run_num
        for onset in par_food_onset_TRs[run_num]:
            offset = onset + 9  #Get block offset -- note: this will be the first TR after the block of interest
            run_food_TR_list[int(onset):int(offset)] = [1, 1, 1, 1, 1, 1, 1, 1, 1]  #At indices onset to offset-1 in r_int_list, set value to 1

        # add run data to food_TR_list
        food_TR_list.extend(run_food_TR_list)

    return(food_TR_list)

def _gen_sub_cen_sum(food_TR_list, sub_censor_list):

    if len(food_TR_list) != len(sub_censor_list):
        print("lengths of food_TR_list and sub_censor_list do not match")
        raise Exception()

    n_total = len(sub_censor_list)
    n_uncensored = sub_censor_list.count(1)

    n_food_uncensored = 0
    for i in range(len(sub_censor_list)):
        if food_TR_list[i] == 1:
            if sub_censor_list[i] == 1:
               n_food_uncensored = n_food_uncensored + 1 

    return n_total, n_uncensored, n_food_uncensored


def censor_sum(par_id, censor_str='rmsd-0.3_c-ba', overwrite = False, preproc_path = False):

    # get script location
    script_path = Path(__file__).parent.resolve()

    # change directory to base directory (BIDSdat) and get path
    os.chdir(script_path)
    os.chdir('../..')
    base_directory = Path(os.getcwd())

    #set specific paths
    bids_fmriprep_path = Path(base_directory).joinpath('derivatives/preprocessed/fmriprep')
    bids_raw_path = Path(base_directory).joinpath('raw_data')

    # set sub with leading zeros
    sub = str(par_id).zfill(3)
   
    # get participant files
    confound_files = list(Path(bids_fmriprep_path).rglob('sub-' + str(sub) + '/ses-1/func/*task-foodcue_run*confounds_timeseries.tsv'))
    eventsfiles = list(Path(bids_raw_path).rglob('sub-' + str(sub) + '/ses-1/func/*foodcue*events.tsv'))
    sub_censor_file = Path(bids_fmriprep_path).joinpath('sub-' + sub + '/ses-1/func/F31_sub-' + sub + '_foodcue-allruns_censor_' + str(censor_str) + '.tsv')

    # import sub_censor_file or raise exception
    if  sub_censor_file.is_file():
        # import database --- converting 'sub' to string will maintain leading zeros
        sub_censor_dat = pd.read_csv(str(sub_censor_file), sep = '\t')
        # convert to list
        sub_censor_list = sub_censor_dat['header'].tolist()
    else:
        print('F31_sub-' + sub + '_foodcue-allruns_censor_' + str(censor_str) + '.tsv does not exist. run p2_create_censor_file')
        raise Exception()


    # load or generate censory summary dataframe
    censor_summary = _get_summary_file(bids_fmriprep_path, censor_str, sub, overwrite)

    # get food block TR onsets
    par_food_onset_TRs = _get_food_onsetTRs(eventsfiles)

    # generate list that indexes which TRs are food (1) and non-food (0)
    food_TR_list = _gen_food_TR_list(par_food_onset_TRs, confound_files)

    # get subject censor summary information
    n_total, n_uncensored, n_food_uncensored = _gen_sub_cen_sum(food_TR_list, sub_censor_list)

    # add subject censor summary information to summary database
    row = pd.DataFrame([[sub, censor_str, n_total, n_uncensored, n_food_uncensored]], columns=['sub','censor_str', 'n_total_trs', 'n_total_trs_uncensored', 'n_food_trs_uncensored'])
    censor_summary = pd.concat([censor_summary, row])

    # export censor summary database
    censor_summary.to_csv(str(Path(bids_fmriprep_path).joinpath('task-foodcue_F31-censorsummary.tsv')), sep = '\t', encoding='utf-8-sig', index = False, header=True)
