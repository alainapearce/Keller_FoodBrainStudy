[
    [
        "Space_group_trialdat",
        [
            "no files"
        ]
    ],
    [
        "Space_summary_dat",
        [
            "no files"
        ]
    ],
    [
        "bids_dir",
        "/Users/azp271/OneDrive - The Pennsylvania State University/b-childfoodlab_Shared/Active_Studies/RO1_Brain_Mechanisms_IRB_5357/Participant_Data/BIDSdat"
    ],
    [
        "function_str",
        "def updateDatabase_save(Space_summary_dat, Space_group_trialdat, overwrite_flag, bids_dir):\n    import pandas as pd\n    import numpy as np\n    from pathlib import Path\n    from nipype.interfaces.base import Bunch\n\n    #derivative data path\n    derivative_data_path = Path(bids_dir).joinpath('derivatives/preprocessed/beh')\n\n    #function to drop rows based on values\n    def filter_rows_by_values(df, sub_values, sesnum):\n        #filter based on sub and ses\n        return df[(df['sub'].isin(sub_values) == False) & (df['ses'] == sesnum)]\n\n    #### Summary Data ####\n    #check to see if it is filepath str or 'no files' message\n    if isinstance(Space_summary_dat[0], str):\n\n        print('******** No new data to be processed ********')\n\n        Space_database_wide = 'no new data files'\n        Space_database_blocks = 'no new data files'\n\n    else:\n        #get a Bunch object if more than 1 participant\n        if isinstance(Space_summary_dat, Bunch):\n            #get output data from node\n            Space_summary_datlist = Space_summary_dat.summarySpace_dat\n\n            #combine datasets\n            Space_summary_dat = pd.concat(Space_summary_datlist)\n\n        #if only 1 participant/dataset then it is a list\n        elif isinstance(Space_summary_dat, list):\n            if len(Space_summary_dat) == 1:\n                Space_summary_dat = Space_summary_dat[0]\n            else:\n                Space_summary_dat = pd.concat(Space_summary_dat)\n\n        #if a pandas dataframe\n        if isinstance(Space_summary_dat, pd.DataFrame):\n\n            #get column names\n            columnnames = Space_summary_dat.columns\n\n            #get session subsets\n            db_sessions = Space_summary_dat.ses.unique()\n\n            if len(db_sessions) > 1:\n                Space_sum_ses1_dat = Space_summary_dat.groupby('ses').get_group(1)\n                Space_sum_ses2_dat = Space_summary_dat.groupby('ses').get_group(2)\n\n                #make wide data set\n                Space_sum_ses1_wide = Space_sum_ses1_dat.pivot(columns='block', index='sub', values=columnnames[3:16])\n                Space_sum_ses1_wide.columns = ['_'.join(col) for col in Space_sum_ses1_wide.columns.reorder_levels(order=[1, 0])]\n\n                Space_sum_ses2_wide = Space_sum_ses2_dat.pivot(columns='block', index='sub', values=columnnames[3:16])\n                Space_sum_ses2_wide.columns = ['_'.join(col) for col in Space_sum_ses2_wide.columns.reorder_levels(order=[1, 0])]\n\n                #make the sub index into a dataset column\n                Space_sum_ses1_wide = Space_sum_ses1_wide.reset_index(level = 0)\n                Space_sum_ses2_wide = Space_sum_ses2_wide.reset_index(level = 0)\n\n                #add session\n                Space_sum_ses1_wide.insert(1, 'ses', 1)\n                Space_sum_ses2_wide.insert(1, 'ses', 2)\n\n                #concatonate databases\n                Space_summary_wide = pd.concat([Space_sum_ses1_wide,Space_sum_ses2_wide],ignore_index=True)\n\n            else:\n                #make wide data set\n                Space_summary_wide = Space_summary_dat.pivot(columns='block', index='sub', values=columnnames[3:16])\n                Space_summary_wide.columns = ['_'.join(col) for col in Space_summary_wide.columns.reorder_levels(order=[1, 0])]\n\n                #make the sub index into a dataset column\n                Space_summary_wide = Space_summary_wide.reset_index(level = 0)\n\n                #add session\n                Space_summary_wide.insert(1, 'ses', db_sessions[0])\n\n            #re-order columns\n            columnnames_reorder = ['sub', 'ses', 'all_earth_rt_mean', 'all_earth_rt_median', 'all_earth_n_miss', 'all_earth_p_miss', 'all_planet_rt_mean', 'all_planet_rt_median', 'all_planet_n_miss',  'all_planet_p_miss', 'all_reward_rate', 'all_avg_reward', 'all_reward_rate_corrected',  'all_prob_sameplanet_earthsame', 'all_prob_sameplanet_earthdif', 'b1_earth_rt_mean', 'b1_earth_rt_median', 'b1_earth_n_miss', 'b1_earth_p_miss', 'b1_planet_rt_mean', 'b1_planet_rt_median', 'b1_planet_n_miss', 'b1_planet_p_miss', 'b1_reward_rate', 'b1_avg_reward', 'b1_reward_rate_corrected', 'b1_prob_sameplanet_earthsame','b1_prob_sameplanet_earthdif', 'b2_earth_rt_mean', 'b2_earth_rt_median', 'b2_earth_n_miss', 'b2_earth_p_miss', 'b2_planet_rt_mean', 'b2_planet_rt_median', 'b2_planet_n_miss', 'b2_planet_p_miss', 'b2_reward_rate', 'b2_avg_reward', 'b2_reward_rate_corrected', 'b2_prob_sameplanet_earthsame', 'b2_prob_sameplanet_earthdif', 'b3_earth_rt_mean', 'b3_earth_rt_median', 'b3_earth_n_miss', 'b3_earth_p_miss', 'b3_planet_rt_mean','b3_planet_rt_median', 'b3_planet_n_miss', 'b3_planet_p_miss', 'b3_reward_rate', 'b3_avg_reward', 'b3_reward_rate_corrected', 'b3_prob_sameplanet_earthsame', 'b3_prob_sameplanet_earthdif', 'b4_earth_rt_mean', 'b4_earth_rt_median', 'b4_earth_n_miss', 'b4_earth_p_miss',  'b4_planet_rt_mean', 'b4_planet_rt_median', 'b4_planet_n_miss', 'b4_planet_p_miss',   'b4_reward_rate', 'b4_avg_reward', 'b4_reward_rate_corrected', 'b4_prob_sameplanet_earthsame', 'b4_prob_sameplanet_earthdif']\n\n            Space_summary_wide = Space_summary_wide.reindex(columns=columnnames_reorder)\n\n            #get blocks subset\n            Space_summary_blocks = Space_summary_dat[Space_summary_dat.block.isin(['b1', 'b2', 'b3', 'b4'])]\n\n            #load databases\n            Space_database_wide = pd.read_csv(str(Path(derivative_data_path).joinpath('task-space_summary.tsv')), sep = '\\t')\n            Space_database_blocks = pd.read_csv(str(Path(derivative_data_path).joinpath('task-space_summary_long.tsv')), sep = '\\t')\n\n            #if overwriting participants\n            if overwrite_flag == True:\n                #filter out/remove exisiting subs to overwrit~\n                if len(db_sessions) > 1:\n                    #get list of subs by ses to filter in wide and long data\n                    wide_sub_list = Space_summary_wide.groupby('ses')['sub'].unique()\n                    long_sub_list = Space_summary_blocks.groupby('ses')['sub'].unique()\n\n                    Space_database_ses1 = filter_rows_by_values(Space_database_wide, wide_sub_list[0], 1)\n                    Space_database_ses2 = filter_rows_by_values(Space_database_wide, wide_sub_list[1], 2)\n\n                    Space_database_ses1_long = filter_rows_by_values(Space_database_blocks, long_sub_list[0], 1)\n                    Space_database_ses2_long = filter_rows_by_values(Space_database_blocks, long_sub_list[1], 2)\n\n                    #concatonate databases\n                    Space_database_wide = pd.concat([Space_database_ses1, Space_database_ses2],ignore_index=True)\n                    Space_database_blocks = pd.concat([Space_database_ses1_long, Space_database_ses2_long],ignore_index=True)\n\n                else:\n                    wide_sub_list = list(Space_summary_wide['sub'].unique())\n                    long_sub_list = list(Space_summary_blocks['sub'].unique())\n\n                    #filter by ses and sub\n                    Space_database_ses = filter_rows_by_values(Space_database_wide, wide_sub_list, db_sessions[0])\n                    Space_database_long_ses = filter_rows_by_values(Space_database_blocks, long_sub_list, db_sessions[0])\n\n                    #concatonate with other session in full database\n                    Space_database_wide = pd.concat([Space_database_wide[Space_database_wide['ses'] != db_sessions[0]], Space_database_ses],ignore_index=True)\n                    Space_database_blocks = pd.concat([Space_database_blocks[Space_database_blocks['ses'] != db_sessions[0]], Space_database_long_ses],ignore_index=True)\n\n\n            #add newly processed data\n            Space_database_wide = Space_database_wide.append(Space_summary_wide)\n            Space_database_blocks = Space_database_blocks.append(Space_summary_blocks)\n\n            #sort to ensure in sub order\n            Space_database_wide = Space_database_wide.sort_values(by = ['ses', 'sub'])\n            Space_database_blocks = Space_database_blocks.sort_values(by = ['ses', 'sub', 'block'])\n\n            #round to 3 decimal points\n            Space_database_wide = Space_database_wide.applymap(lambda x: round(x, 3) if isinstance(x, (int, float)) else x)\n            Space_database_blocks = Space_database_blocks.applymap(lambda x: round(x, 3) if isinstance(x, (int, float)) else x)\n\n            #write databases\n            Space_database_wide.to_csv(str(Path(derivative_data_path).joinpath('task-space_summary.tsv')), sep = '\\t', encoding='utf-8-sig', index = False)\n            Space_database_blocks.to_csv(str(Path(derivative_data_path).joinpath('task-space_summary_long.tsv')), sep = '\\t', encoding='utf-8-sig', index = False)\n\n        else:\n            print('No raw data files that need to be processed')\n            Space_database_wide = np.nan\n            Space_database_blocks = np.nan\n\n    #### Group trial data ####\n    if isinstance(Space_group_trialdat[0], str):\n\n        print('******** No new data to be processed ********')\n\n        Space_groupdat = 'no new data files'\n\n    else:\n        #get a Bunch object if more than 1 participant\n        if isinstance(Space_group_trialdat, Bunch):\n            #get output data from node\n            Space_group_trialdatlist = Space_group_trialdat.group_trialdat\n\n            #combine datasets\n            Space_groupdat = pd.concat(Space_group_trialdatlist)\n\n        #if only 1 participant/dataset then it is a list\n        elif isinstance(Space_group_trialdat, list):\n            if len(Space_group_trialdat) == 1:\n                Space_groupdat = Space_group_trialdat[0]\n            else:\n                Space_groupdat = pd.concat(Space_group_trialdat)\n\n        #if a pandas dataframe\n        if isinstance(Space_groupdat, pd.DataFrame):\n\n            #get session subsets\n            db_group_sessions = Space_groupdat.ses.unique()\n\n            #load databases\n            Space_groupdat_database = pd.read_csv(str(Path(derivative_data_path).joinpath('task-space_groupdata.tsv')), sep = '\\t')\n\n            #if overwriting participants\n            if overwrite_flag == True:\n                #filter out/remove exisiting subs to overwrit~\n                if len(db_group_sessions) > 1:\n                    #get list of subs by ses to filter in wide and long data\n                    dat_sub_list = Space_groupdat.groupby('ses')['sub'].unique()\n\n                    Space_groupdat_ses1 = filter_rows_by_values(Space_groupdat_database, dat_sub_list[0], 1)\n                    Space_groupdat_ses2 = filter_rows_by_values(Space_groupdat_database, dat_sub_list[1], 2)\n\n                    #concatonate databases\n                    Space_groupdat_database = pd.concat([Space_groupdat_ses1, Space_groupdat_ses2],ignore_index=True)\n\n                else:\n                    dat_sub_list = list(Space_groupdat['sub'].unique())\n\n                    #filter by ses and sub\n                    Space_groupdat_ses = filter_rows_by_values(Space_groupdat_database, dat_sub_list, db_group_sessions[0])\n\n                    #concatonate with other session in full database\n                    Space_groupdat_database = pd.concat([Space_groupdat_database[Space_groupdat_database['ses'] != db_group_sessions[0]], Space_groupdat_ses],ignore_index=True)\n\n            #add newly processed data\n            Space_groupdat_database = Space_groupdat_database.append(Space_groupdat)\n\n            #sort to ensure in sub order\n            Space_groupdat_database = Space_groupdat_database.sort_values(by = ['sub', 'ses'])\n\n            #round to 3 decimal points\n            Space_groupdat_database = Space_groupdat_database.applymap(lambda x: round(x, 3) if isinstance(x, (int, float)) else x)\n\n            #write databases\n            Space_groupdat_database.to_csv(str(Path(derivative_data_path).joinpath('task-space_groupdata.tsv')), sep = '\\t', encoding='utf-8-sig', index = False)\n\n        else:\n            print('No raw trial data files that need to be processed')\n            Space_groupdat = np.nan\n\n    return Space_database_wide, Space_database_blocks, Space_groupdat\n"
    ],
    [
        "overwrite_flag",
        false
    ]
]