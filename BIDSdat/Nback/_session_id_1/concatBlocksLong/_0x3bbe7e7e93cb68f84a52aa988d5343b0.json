[
    [
        "bids_dir",
        "/Users/azp271/OneDrive - The Pennsylvania State University/b-childfoodlab_Shared/Active_Studies/RO1_Brain_Mechanisms_IRB_5357/Participant_Data/BIDSdat"
    ],
    [
        "block_sumDat",
        [
            "no files"
        ]
    ],
    [
        "function_str",
        "def updateDatabase_save(block_sumDat, overwrite_flag, bids_dir):\n    import numpy as np\n    import pandas as pd\n    from pathlib import Path\n    from nipype.interfaces.base import Bunch\n\n    #check to see if it is filepath str or 'no files' message\n    if isinstance(block_sumDat[0], str):\n\n        print('******** No new data to be processed ********')\n\n        Nback_database = 'no new data files'\n        Nback_database_long = 'no new data files'\n\n    else:\n        #get a Bunch object if more than 1 participant\n        if isinstance(block_sumDat, Bunch):\n            #get output data from node\n            np_allBlockDat = block_sumDat.summaryNback_dat\n\n        #if only 1 participant/dataset then it is a list\n        elif isinstance(block_sumDat, list):\n            if len(block_sumDat) == 1:\n                np_allBlockDat = block_sumDat[0]\n            else:\n                np_allBlockDat = block_sumDat\n\n        #convert np subarrays to pandas\n        def np2pds(t):\n            return [pd.DataFrame(sublist) for sublist in t]\n\n        pandas_allBlockDat = np2pds(np_allBlockDat)\n\n        #combine datasets\n        allBlockDat = pd.concat(pandas_allBlockDat)\n\n        #if a pandas dataframe\n        if isinstance(allBlockDat, pd.DataFrame):\n            col_names = ['sub', 'ses', 'block','n_targets', 'n_fill', 'n_trials', 'n_acc', 'p_acc', 'n_target_hit', 'p_target_hit', 'n_target_miss', 'p_target_miss', 'n_fill_corr', 'p_fill_corr', 'n_fill_fa', 'p_fill_fa', 'p_target_ba', 'rt_mean_target_hit', 'rt_med_target_hit']\n            allBlockDat.columns = col_names\n            allBlockDat = pd.DataFrame(allBlockDat).convert_dtypes()\n            allBlockDat = allBlockDat.reset_index(drop = True)\n\n            #set numeric columns to dtype numeric\n            num_cols = allBlockDat.loc[:, allBlockDat.columns != 'block'].apply(pd.to_numeric).round(3)\n\n            #replace in orig dataset\n            allBlockDat.loc[:, num_cols.columns] = num_cols\n\n            #get session subsets\n            db_sessions = allBlockDat.ses.unique()\n\n            #make wide data set\n            if len(db_sessions) > 1:\n                allBlockDat_ses1_dat = allBlockDat.groupby('ses').get_group(1)\n                allBlockDat_ses2_dat = allBlockDat.groupby('ses').get_group(2)\n\n                #make wide data set\n                allBlockDat_ses1_wide = allBlockDat_ses1_dat.pivot(columns='block', index='sub', values=col_names[3:19])\n                allBlockDat_ses1_wide.columns = ['_'.join(col) for col in allBlockDat_ses1_wide.columns.reorder_levels(order=[1, 0])]\n\n                allBlockDat_ses2_wide = allBlockDat_ses2_dat.pivot(columns='block', index='sub', values=col_names[3:19])\n                allBlockDat_ses2_wide.columns = ['_'.join(col) for col in allBlockDat_ses2_wide.columns.reorder_levels(order=[1, 0])]\n\n                #make the sub index into a dataset column\n                allBlockDat_ses1_wide = allBlockDat_ses1_wide.reset_index(level = 0)\n                allBlockDat_ses2_wide = allBlockDat_ses2_wide.reset_index(level = 0)\n\n                #add session\n                allBlockDat_ses1_wide.insert(1, 'ses', 1)\n                allBlockDat_ses1_wide.insert(1, 'ses', 2)\n\n\n                #concatonate databases\n                allBlockDat_wide = pd.concat([allBlockDat_ses1_wide, allBlockDat_ses2_wide],ignore_index=True)\n\n            else:\n                #make wide data set\n                allBlockDat_wide = allBlockDat.pivot(columns='block', index='sub', values = col_names[3:19])\n                allBlockDat_wide.columns = ['_'.join(col) for col in allBlockDat_wide.columns.reorder_levels(order=[1, 0])]\n\n                #make the sub index into a dataset column\n                allBlockDat_wide = allBlockDat_wide.reset_index(level = 0)\n\n                #add session\n                allBlockDat_wide.insert(1, 'ses', db_sessions[0])\n\n            #re-order columns\n            columnnames_reorder = ['sub', 'ses',\n            'b0_n_targets', 'b0_n_fill', 'b0_n_trials', 'b0_n_acc','b0_p_acc',\n            'b0_n_target_hit','b0_p_target_hit', 'b0_n_target_miss',\n            'b0_p_target_miss','b0_n_fill_corr','b0_p_fill_corr',\n            'b0_n_fill_fa', 'b0_p_fill_fa','b0_p_target_ba',\n            'b0_rt_mean_target_hit','b0_rt_med_target_hit',\n            'b1_n_targets', 'b1_n_fill', 'b1_n_trials', 'b1_n_acc','b1_p_acc',\n            'b1_n_target_hit','b1_p_target_hit','b1_n_target_miss',\n            'b1_p_target_miss', 'b1_n_fill_corr','b1_p_fill_corr',\n            'b1_n_fill_fa','b1_p_fill_fa','b1_p_target_ba',\n            'b1_rt_mean_target_hit','b1_rt_med_target_hit',\n            'b2_n_targets', 'b2_n_fill', 'b2_n_trials', 'b2_n_acc','b2_p_acc',\n            'b2_n_target_hit','b2_p_target_hit', 'b2_n_target_miss',\n            'b2_p_target_miss','b2_n_fill_corr','b2_p_fill_corr',\n            'b2_n_fill_fa','b2_p_fill_fa','b2_p_target_ba',\n            'b2_rt_mean_target_hit','b2_rt_med_target_hit']\n\n            allBlockDat_wide = allBlockDat_wide.reindex(columns=columnnames_reorder)\n\n            ## load databases\n            #derivative data path\n            derivative_data_path = Path(bids_dir).joinpath('derivatives/preprocessed/beh')\n\n            #load databases\n            Nback_database = pd.read_csv(str(Path(derivative_data_path).joinpath('task-nback_summary.tsv')), sep = '\\t')\n            Nback_database_long = pd.read_csv(str(Path(derivative_data_path).joinpath('task-nback_summary_long.tsv')), sep = '\\t')\n\n            #if overwriting participants\n            if overwrite_flag == True:\n                #function to drop rows based on values\n                def filter_rows_by_values(df, sub_values, sesnum):\n                    #filter based on sub and ses\n                    return df[(df['sub'].isin(sub_values) == False) & (df['ses'] == sesnum)]\n\n                #filter out/remove exisiting subs to overwrit~\n                if len(db_sessions) > 1:\n                    #get list of subs by ses to filter in wide and long data\n                    wide_sub_list = allBlockDat_wide.groupby('ses')['sub'].unique()\n                    long_sub_list = allBlockDat.groupby('ses')['sub'].unique()\n\n                    Nback_database_ses1 = filter_rows_by_values(Nback_database, wide_sub_list[0], 1)\n                    Nback_database_ses2 = filter_rows_by_values(Nback_database, wide_sub_list[1], 2)\n\n                    Nback_database_ses1_long = filter_rows_by_values(Nback_database_long, long_sub_list[0], 1)\n                    Nback_database_ses2_long = filter_rows_by_values(Nback_database_long, long_sub_list[1], 2)\n\n                    #concatonate databases\n                    Nback_database = pd.concat([Nback_database_ses1, Nback_database_ses2],ignore_index=True)\n                    Nback_database_long = pd.concat([Nback_database_ses1_long, Nback_database_ses2_long],ignore_index=True)\n\n                else:\n                    wide_sub_list = list(allBlockDat_wide['sub'].unique())\n                    long_sub_list = list(allBlockDat['sub'].unique())\n\n                    #filter by ses and sub\n                    Nback_database_ses = filter_rows_by_values(Nback_database, wide_sub_list, db_sessions[0])\n                    Nback_database_long_ses = filter_rows_by_values(Nback_database_long, long_sub_list, db_sessions[0])\n\n                    #concatonate with other session in full database\n                    Nback_database = pd.concat([Nback_database[Nback_database['ses'] != db_sessions[0]], Nback_database_ses],ignore_index=True)\n                    Nback_database_long = pd.concat([Nback_database_long[Nback_database_long['ses'] != db_sessions[0]], Nback_database_long_ses],ignore_index=True)\n\n            #add newly processed data\n            Nback_database = Nback_database.append(allBlockDat_wide)\n            Nback_database_long = Nback_database_long.append(allBlockDat)\n\n            #sort to ensure in sub order\n            Nback_database = Nback_database.sort_values(by = ['ses', 'sub'])\n            Nback_database_long = Nback_database_long.sort_values(by = ['ses', 'sub', 'block'])\n\n            #round to 3 decimal points\n            Nback_database = Nback_database.applymap(lambda x: round(x, 3) if isinstance(x, (int, float)) else x)\n            Nback_database_long = Nback_database_long.applymap(lambda x: round(x, 3) if isinstance(x, (int, float)) else x)\n\n            #write databases\n            Nback_database.to_csv(str(Path(derivative_data_path).joinpath('task-nback_summary.tsv')), sep = '\\t', encoding='utf-8-sig', index = False)\n            Nback_database_long.to_csv(str(Path(derivative_data_path).joinpath('task-nback_summary_long.tsv')), sep = '\\t', encoding='utf-8-sig', index = False)\n        else:\n            Nback_database = 'allBlockDat no pd.DataFrame'\n            Nback_database_long = 'allBlockDat no pd.DataFrame'\n\n    return Nback_database, Nback_database_long\n"
    ],
    [
        "overwrite_flag",
        false
    ]
]